{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Data Analysis - Web Mining\n",
    "\n",
    "The web is an immense source of data, offering valuable insights across various domains. In this assignment, we will explore different ways to process, and analyze web data to uncover meaningful patterns and make informed decisions. Through five practical exercises, we will apply key data mining techniques to real-world scenarios.\n",
    "\n",
    "Each exercise focuses on a specific application:\n",
    "\n",
    "* *Exercise 1* - Clickbait classification\n",
    "* *Exercise 2* - Sentiment analysis on comments\n",
    "* *Exercise 3* - Movie recommendation\n",
    "* *Exercise 4* - Association rules in online shopping\n",
    "* *Exercise 5* - Clustering of mobile apps\n",
    "\n",
    "These exercises will provide hands-on experience in working with real-world data collected from the web, helping you understand its potential for analysis.\n",
    "\n",
    "For this assignment, complete all exercises that are marked in <span style='color:red;font-weight:bold'>red</span>. Please make sure all your cells run correctly (try to *Clear All Outputs* then *Run All* once before submitting). **Check the cells outputs are visibles even for the coding parts**\n",
    "\n",
    "The assignment is due for <span style='color:red;font-weight:bold'>Thursday 27th of March 2025 at 23:59</span>.\n",
    "\n",
    "No report is needed as all questions can be answered directly in this notebook file. You only need to give this notebook file completed on the [Moodle assignment page](https://moodle.msengineering.ch/course/view.php?id=2732). Only one file per group is required for submission.\n",
    "\n",
    "If you have any questions or issues, please contact one of the assistants below:\n",
    "- Cédric Campos Carvalho (*Teams* might be easier to discuss, mail: cedric.camposcarvalho@heig-vd.ch)\n",
    "- Elena Najdenovska (mail: elena.najdenovska@heig-vd.ch)\n",
    "\n",
    "\n",
    "Teacher : \n",
    "- Laura Elena Raileanu <Laura.Raileanu@heig-vd.ch>(mail: Laura.Raileanu@heig-vd.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Clickbait classification\n",
    "\n",
    "The objective of this first part is to model a filter for \"clickbait\" in online news media. Clickbait headlines are designed to attract attention and drive clicks, often at the expense of accuracy or relevance.\n",
    "\n",
    "To achieve this, we provide you with a dataset containing more than 10'000 press headlines collected in 2016. Each row in this dataset corresponds to a single headline, which is described by the following two attributes:\n",
    "\n",
    "* `headline`: the text representing the title\n",
    "* `clickbait`: the label identifying whether the title is a clickbait *(1)* or not *(0)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Need To Tell Us If These Things Are Doughnuts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15 Great Pieces Of Relationship Advice From Books</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improved E-Mail Service From a Dedicated Device</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two MBTA Green Line trains collide in Newton, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 Struggles All Smartypants Will Understand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10536</th>\n",
       "      <td>Can You Match The Phone To The R&amp;B Video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10537</th>\n",
       "      <td>19 Soul Food Recipes That Are Almost As Good A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10538</th>\n",
       "      <td>16 Photos Of Desis That Will Give You Intense ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539</th>\n",
       "      <td>City Plans to Make Older Buildings Refit to Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10540</th>\n",
       "      <td>Iraqis accept constitution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0      You Need To Tell Us If These Things Are Doughnuts          1\n",
       "1      15 Great Pieces Of Relationship Advice From Books          1\n",
       "2        Improved E-Mail Service From a Dedicated Device          0\n",
       "3      Two MBTA Green Line trains collide in Newton, ...          0\n",
       "4           17 Struggles All Smartypants Will Understand          1\n",
       "...                                                  ...        ...\n",
       "10536           Can You Match The Phone To The R&B Video          1\n",
       "10537  19 Soul Food Recipes That Are Almost As Good A...          1\n",
       "10538  16 Photos Of Desis That Will Give You Intense ...          1\n",
       "10539  City Plans to Make Older Buildings Refit to Sa...          0\n",
       "10540                         Iraqis accept constitution          0\n",
       "\n",
       "[10541 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_excel('data/part1_classification/news_clickbait.xlsx', engine=\"openpyxl\")\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.1 :</p>\n",
    "\n",
    "The first step is to separate the dataset into two sets (training and test), complete the input parameters of `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... = train_test_split(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.2 :</p>\n",
    "\n",
    "Create a pre-processing `Pipeline` using [scikit-learn Pipeline object](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "In your pipeline you need:\n",
    "- Vectorize your headlines with Term frequency-inverse document frequency.\n",
    "- Transform your data in case of [`sparse matrix`](https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse), so the data goes through the model without any issues.\n",
    "\n",
    "**Do not forget to remove words giving no information for classification (i.e. [stop words](https://en.wikipedia.org/wiki/Stop_word)).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour convertir une matrice sparse en dense (nécessaire pour GaussianNB)\n",
    "def to_dense(X):\n",
    "    return X.toarray()\n",
    "\n",
    "# Pipeline de pré-traitement\n",
    "preprocessor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),  # Vectorisation TF-IDF + suppression des stop words\n",
    "    ('densify', FunctionTransformer(to_dense, accept_sparse=True)), # Conversion en matrice dense\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.3 :</p>\n",
    "\n",
    "Create the pipeline using `make_pipeline`, combining the pre-processing pipeline and the `GaussianNB` model. Then, train the pipeline and find the score obtained with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.3\n",
    "# Création du pipeline complet (pré-traitement + modèle)\n",
    "pipeline = make_pipeline(preprocessor, GaussianNB())\n",
    "\n",
    "# Entraînement du pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation sur l'ensemble de test\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Score (accuracy) sur l'ensemble de test : {score}\")\n",
    "\n",
    "# Prédictions sur l'ensemble de test (pour la suite)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.4 :</p>\n",
    "\n",
    "Modify your split ratio for the training/testing set and see if there's a difference in the model's performances. Please explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tester différents ratios de split\n",
    "for test_size in [0.1, 0.2, 0.3, 0.4]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print(f\"Test size: {test_size}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.5 :</p>\n",
    "\n",
    "Keep your split validation but now incorporate [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). Use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function and a K-Folding with 5 splits over the training set without using the test set. Then, calculate the averaged accuracy (with standard deviation) for 5 folds.\n",
    "\n",
    "**Explain the results obtained and how to read them compared to the split validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.5\n",
    "# Validation croisée avec 5 folds\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Scores de validation croisée : {cv_scores}\")\n",
    "print(f\"Moyenne des scores : {cv_scores.mean()}\")\n",
    "print(f\"Écart-type des scores : {cv_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.6 :</p>\n",
    "\n",
    "Try to use atleast 3 different classifiers and report their results in a table. Compare the results between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.6\n",
    "# Dictionnaire des classifieurs\n",
    "classifiers = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "# Stocker les résultats pour chaque classifieur\n",
    "results = {}\n",
    "\n",
    "# Boucle sur les classifieurs\n",
    "for name, clf in classifiers.items():\n",
    "    # Créer le pipeline\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # Validation croisée\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Stocker les résultats\n",
    "    results[name] = {\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_accuracy': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Afficher les résultats sous forme de tableau\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.7 :</p>\n",
    "\n",
    "In text processing we often use a stemming step for the pre-processing, explain what it consists and how it can be useful then give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Sentiment analysis on comments\n",
    "\n",
    "The goal of the second part is to analyze tweets from the COVID-19 period and perform sentiment analysis to determine whether they express a positive or negative sentiment.\n",
    "\n",
    "In the first step, we will work with the dataset `CoronaTwitterComments_2labels.xlsx`, which contains approximately 3'000 comments related to COVID-19 from Twitter in March 2020. These comments are already labeled with *1* (positive comment) or *-1* (negative comment). In the second step, we will consider the dataset `CoronaTwitterComments_3labels.xlsx`, which includes additional neutral comments (labeled with *0*). You will see how we can process the text data with the help of [WordNet](https://wordnet.princeton.edu/) to retrieve a sentiment and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Cedric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Cedric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Cedric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Cedric\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK packages needed for this exercise, feel free to add some if you need it.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [-1  1  0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>SentimentLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>@RicePolitics @MDCounties Craig, will you call...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OriginalTweet  SentimentLabel\n",
       "0     TRENDING: New Yorkers encounter empty supermar...              -1\n",
       "1     When I couldn't find hand sanitizer at Fred Me...               1\n",
       "2     Find out how you can protect yourself and love...               1\n",
       "3     #Panic buying hits #NewYork City as anxious sh...              -1\n",
       "4     Voting in the age of #coronavirus = hand sanit...               1\n",
       "...                                                 ...             ...\n",
       "3174  @RicePolitics @MDCounties Craig, will you call...              -1\n",
       "3175  Meanwhile In A Supermarket in Israel -- People...               1\n",
       "3176  Did you panic buy a lot of non-perishable item...              -1\n",
       "3177  Gov need to do somethings instead of biar je r...              -1\n",
       "3178  I and @ForestandPaper members are committed to...               1\n",
       "\n",
       "[3179 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_tweets2 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_2labels.xlsx', engine=\"openpyxl\")\n",
    "corona_tweets3 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_3labels.xlsx', engine=\"openpyxl\")\n",
    "\n",
    "print(corona_tweets2.SentimentLabel.unique(), corona_tweets3.SentimentLabel.unique())\n",
    "corona_tweets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.1 :</p>\n",
    "\n",
    "The first step is to pre-process the text (like in previous exercise) to later use a WordNet sentiment analysis over it.\n",
    "\n",
    "The class inherits from `BaseEstimator` and `TransformerMixin`, ensuring seamless integration with other scikit-learn modules. This allows it to utilize the familiar `fit`, `transform`, and `fit_transform` functions of the library.\n",
    "\n",
    "Your task is to update the `NLTKPreprocessor` class with the needed functions to be used later in the `transform` function.\n",
    "- Add a tokenizer specialized in tweets.\n",
    "- Remove stop words and other unuseful characters.\n",
    "- Transform in lowercase the text\n",
    "- Lemmatize the text using for Wordnet.\n",
    "\n",
    "*Advice : If you encounter any issues, refer to the `apply_pipeline` function to understand how it works. This function should **not** be modified.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # TODO        \n",
    "\n",
    "        self.pipeline = [\n",
    "            # TODO\n",
    "        ]\n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.2 :</p>\n",
    "\n",
    "The next step is to create the `WordNetSentimentAnalyzer`. You will need to create three functions to add to your pipeline : \n",
    "1. The WordNet sentiment analysis requires to have the tag of each word of the sentence. Tagging in part of speech (POS) is the process of assigning grammatical categories, such as nouns, verbs, or adjectives, to words in a text based on their role in a sentence. `nltk` has a pre-trained model that can tag these words, find it and apply it to the pipeline.\n",
    "2. For WordNet the tagging is different from the pre-trained `nltk` model. Create a function replacing the tags with the WordNet tags using `nltk.corpus.wordnet` module.\n",
    "3. Create the sentiment function which sums the positive sentiment ($\\text{pos}$) and negative sentiment ($\\text{neg}$) of each word (using `nltk.corupus.sentiwordnet` module). The value needs to be taken in account only if it is superior to a certain threshold (i.e. $\\text{treshold} = 0.05$). Then, return a single value representing the sentiment of the sentence such as it is $\\text{sentiment} < 0$ if it's negative and $\\text{sentiment} > 0$ if positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetSentimentAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # TODO\n",
    "\n",
    "        self.pipeline = [\n",
    "            # TODO\n",
    "        ]\n",
    "\n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.3 :</p>\n",
    "\n",
    "Create two functions that return the sentiment class to compare it to the labels.\n",
    "* `get_sentiment_class_2` : For positive and negative only.\n",
    "* `get_sentiment_class_3` : For positive, negative and neutral.\n",
    "\n",
    "**Do not forget that it is going to be used in a `scikit-learn` `Pipeline`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_class_2(sent_val):\n",
    "    return # TODO\n",
    "\n",
    "def get_sentiment_class_3(sent_val):\n",
    "    return # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.4 :</p>\n",
    "\n",
    "Create a `scikit-learn` `Pipeline` using the 3 previous *Transformers* that you created (using the 2 class sentiment).\n",
    "\n",
    "Then, complete the function `get_sentiment_tweet` and test it with your own Tweet (**Do not change its signature !**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    # TODO\n",
    "])\n",
    "\n",
    "def get_sentiment_tweet(tweet:str)-> int:\n",
    "    return # TODO\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.5 :</p>\n",
    "\n",
    "Compute the accuracy obtained with all tweets of the dataset. Then, do the same for the 3 labels class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.6 :</p>\n",
    "\n",
    "Analyze the results of each pre-processing (`NLTKPreprocessor`) step to try to understand what you could do to improve it. For example, what could you add into this pipeline ? *(No implementation needed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Movie recommendation\n",
    "\n",
    "The objective of this exercise is to create a recommendation system of movies based on the user ratings. We will focus on the collaborative approach for movie recommendation using the provided dataset, which contains approximately 9'800 movies rated by 610 users from MovieLens.\n",
    "\n",
    "More specifically, the `movies.csv` dataset, which describes the movies, includes three attributes:\n",
    "\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `title`: the title of the movie (with the release year in parentheses)\n",
    "* `genres`: the genres of the movie\n",
    "\n",
    "The `ratings.csv` dataset, which contains user ratings for the movies, includes four attributes:\n",
    "\n",
    "* `userId`: unique identifier of the user\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `rating`: the user's rating for the corresponding movie\n",
    "* `timestamp`: the timestamp of the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>847434962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1106635946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1510577970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1305696483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "      <td>184</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1537109082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537109545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537109805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537110021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1537157606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        movieId                                      title  \\\n",
       "0             1                           Toy Story (1995)   \n",
       "1             1                           Toy Story (1995)   \n",
       "2             1                           Toy Story (1995)   \n",
       "3             1                           Toy Story (1995)   \n",
       "4             1                           Toy Story (1995)   \n",
       "...         ...                                        ...   \n",
       "100831   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "100832   193583               No Game No Life: Zero (2017)   \n",
       "100833   193585                               Flint (2017)   \n",
       "100834   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "100835   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                             genres  userId  rating  \\\n",
       "0       Adventure|Animation|Children|Comedy|Fantasy       1     4.0   \n",
       "1       Adventure|Animation|Children|Comedy|Fantasy       5     4.0   \n",
       "2       Adventure|Animation|Children|Comedy|Fantasy       7     4.5   \n",
       "3       Adventure|Animation|Children|Comedy|Fantasy      15     2.5   \n",
       "4       Adventure|Animation|Children|Comedy|Fantasy      17     4.5   \n",
       "...                                             ...     ...     ...   \n",
       "100831              Action|Animation|Comedy|Fantasy     184     4.0   \n",
       "100832                     Animation|Comedy|Fantasy     184     3.5   \n",
       "100833                                        Drama     184     3.5   \n",
       "100834                             Action|Animation     184     3.5   \n",
       "100835                                       Comedy     331     4.0   \n",
       "\n",
       "         timestamp  \n",
       "0        964982703  \n",
       "1        847434962  \n",
       "2       1106635946  \n",
       "3       1510577970  \n",
       "4       1305696483  \n",
       "...            ...  \n",
       "100831  1537109082  \n",
       "100832  1537109545  \n",
       "100833  1537109805  \n",
       "100834  1537110021  \n",
       "100835  1537157606  \n",
       "\n",
       "[100836 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies = pd.read_csv('data/part3_recommandationdesfilms/movies.csv')\n",
    "df_ratings = pd.read_csv('data/part3_recommandationdesfilms/ratings.csv')\n",
    "\n",
    "df_3 = pd.merge(df_movies, df_ratings, on='movieId')\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.1 :</p>\n",
    "\n",
    "Use `df_3` to create a second `DataFrame` with the rating of every movies for each `userId`. If the user never watched a movie the value is $0$. \n",
    "For this step, use the [`pivot` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = ... # TODO 3.1\n",
    "X = user_item_matrix.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.2 :</p>\n",
    "\n",
    "First separate the data using `train_test_split` function with $20\\%$ of the data in the test set.\n",
    "Create an `knn` model using the [`NearestNeighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) model from `scikit-learn`. For now use $N=80$, and the `cosine` metric, then train it.\n",
    "\n",
    "**Explain in few sentences how does the `cosine` metric works by taking our case in an example (include the formula).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... = train_test_split(...)\n",
    "\n",
    "knn = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 3.2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.3 :</p>\n",
    "\n",
    "Create the `predict` function by using the previous trained model.\n",
    "\n",
    "*Hints : Think about the model returns and what they represent. Then, use the closest indexes to obtain information on the best matching movies.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, user_item_matrix, knn):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.4 :</p>\n",
    "\n",
    "Finally, complete the `prec_at_k` function, which measures the precision of our model. This metric calculates how many of the top $k$ movies recommended to a user are actually relevant. A movie is considered relevant if the user has given it a score of at least 4.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_at_k(Y_pred, user_item_matrix, k):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.5 :</p>\n",
    "\n",
    "Now, use the function with three different values for k ($5$, $15$, $25$) each time. Then, use different $N$ neighbours and a different split ratio.\n",
    "\n",
    "Report all your results bellow and explain the impact of these hyperparameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 3.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Association rules in online shopping\n",
    "\n",
    "In this fourth part, we will focus on a **Market Basket Analysis** problem. The dataset provided contains **online sales transactions** from an e-commerce site over one year. Our goal is to generate **association rules** based on these sales to identify which items are frequently bought together. You can refer to the source mentioned in the **README file** included with the data for more details.  \n",
    "\n",
    "The dataset is structured as follows, with each row representing the details of a product sale:  \n",
    "- `InvoiceNo`: invoice/sale identifier  \n",
    "- `StockCode`: product identifier  \n",
    "- `Description`: purchased product  \n",
    "- `Quantity`: quantity sold  \n",
    "- `InvoiceDate`: order/payment date  \n",
    "- `UnitPrice`: price of the product  \n",
    "- `CustomerID`: customer identifier  \n",
    "- `Country`: customer’s country of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397919</th>\n",
       "      <td>581587</td>\n",
       "      <td>22613</td>\n",
       "      <td>PACK OF 20 SPACEBOY NAPKINS</td>\n",
       "      <td>12</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397920</th>\n",
       "      <td>581587</td>\n",
       "      <td>22899</td>\n",
       "      <td>CHILDREN'S APRON DOLLY GIRL</td>\n",
       "      <td>6</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>2.10</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397921</th>\n",
       "      <td>581587</td>\n",
       "      <td>23254</td>\n",
       "      <td>CHILDRENS CUTLERY DOLLY GIRL</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397922</th>\n",
       "      <td>581587</td>\n",
       "      <td>23255</td>\n",
       "      <td>CHILDRENS CUTLERY CIRCUS PARADE</td>\n",
       "      <td>4</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.15</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397923</th>\n",
       "      <td>581587</td>\n",
       "      <td>22138</td>\n",
       "      <td>BAKING SET 9 PIECE RETROSPOT</td>\n",
       "      <td>3</td>\n",
       "      <td>2011-12-09 12:50:00</td>\n",
       "      <td>4.95</td>\n",
       "      <td>12680</td>\n",
       "      <td>France</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>397924 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0          536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1          536365     71053                  WHITE METAL LANTERN         6   \n",
       "2          536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3          536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4          536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "...           ...       ...                                  ...       ...   \n",
       "397919     581587     22613          PACK OF 20 SPACEBOY NAPKINS        12   \n",
       "397920     581587     22899         CHILDREN'S APRON DOLLY GIRL          6   \n",
       "397921     581587     23254        CHILDRENS CUTLERY DOLLY GIRL          4   \n",
       "397922     581587     23255      CHILDRENS CUTLERY CIRCUS PARADE         4   \n",
       "397923     581587     22138        BAKING SET 9 PIECE RETROSPOT          3   \n",
       "\n",
       "               InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0      2010-12-01 08:26:00       2.55       17850  United Kingdom  \n",
       "1      2010-12-01 08:26:00       3.39       17850  United Kingdom  \n",
       "2      2010-12-01 08:26:00       2.75       17850  United Kingdom  \n",
       "3      2010-12-01 08:26:00       3.39       17850  United Kingdom  \n",
       "4      2010-12-01 08:26:00       3.39       17850  United Kingdom  \n",
       "...                    ...        ...         ...             ...  \n",
       "397919 2011-12-09 12:50:00       0.85       12680          France  \n",
       "397920 2011-12-09 12:50:00       2.10       12680          France  \n",
       "397921 2011-12-09 12:50:00       4.15       12680          France  \n",
       "397922 2011-12-09 12:50:00       4.15       12680          France  \n",
       "397923 2011-12-09 12:50:00       4.95       12680          France  \n",
       "\n",
       "[397924 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_4 = pd.read_excel('data/part4_marketbasketanalysis/OnlineRetailDataset.xlsx', engine='openpyxl')\n",
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.1 :</p>\n",
    "\n",
    "Transform the given dataset into a one-hot encoded format for association rule mining. Group items by customer so that each row represents a unique customer with a list of purchased items. Then, convert this into a binary matrix where each column is an item, and values indicate whether a customer bought that item (1) or not (0). Use [`MultiLabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.2 :</p>\n",
    "\n",
    "Finally, create the association rules based on the frequent patterns obtained through `FP-growth` algorithm. Check [`mlxtend`](https://rasbt.github.io/mlxtend/) documentation to complete this task. Then, show what are the association rules with a minimum of $0.9$ confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.3 :</p>\n",
    "\n",
    "Do you observe any changes in the different parameters of the *FP-Growth* and *Association Rules* functions ? Please comment on the chosen parameters and the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.4 :</p>\n",
    "\n",
    "Is it possible to use other column(s) from the initial data to generate interesting rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Clustering of mobile apps\n",
    "\n",
    "In this final part, we will perform clustering of applications from the Google Play Store. You can refer to the source mentioned in the **README file** included with the data for more details.\n",
    "\n",
    "Among the attributes available in the `googleplaystore.xlsx` file, we will use the following:\n",
    "\n",
    "* `Rating`: overall user rating of the application\n",
    "* `Reviews`: number of user reviews\n",
    "* `Size`: size of the application\n",
    "* `Installs`: number of users who installed the application\n",
    "* `Price`: price of the application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Size</th>\n",
       "      <th>Installs</th>\n",
       "      <th>Price</th>\n",
       "      <th>App</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.1</td>\n",
       "      <td>159</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>0</td>\n",
       "      <td>Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.9</td>\n",
       "      <td>967</td>\n",
       "      <td>14.0</td>\n",
       "      <td>500000</td>\n",
       "      <td>0</td>\n",
       "      <td>Coloring book moana</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>87510</td>\n",
       "      <td>8.7</td>\n",
       "      <td>5000000</td>\n",
       "      <td>0</td>\n",
       "      <td>U Launcher Lite – FREE Live Cool Themes, Hide ...</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>215644</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50000000</td>\n",
       "      <td>0</td>\n",
       "      <td>Sketch - Draw &amp; Paint</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.3</td>\n",
       "      <td>967</td>\n",
       "      <td>2.8</td>\n",
       "      <td>100000</td>\n",
       "      <td>0</td>\n",
       "      <td>Pixel Draw - Number Art Coloring Book</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9361</th>\n",
       "      <td>4.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.6</td>\n",
       "      <td>500</td>\n",
       "      <td>0</td>\n",
       "      <td>FR Calculator</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9362</th>\n",
       "      <td>4.5</td>\n",
       "      <td>38</td>\n",
       "      <td>53.0</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>Sya9a Maroc - FR</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9363</th>\n",
       "      <td>5.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>Fr. Mike Schmitz Audio Teachings</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9364</th>\n",
       "      <td>4.5</td>\n",
       "      <td>114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>The SCP Foundation DB fr nn5n</td>\n",
       "      <td>BOOKS_AND_REFERENCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9365</th>\n",
       "      <td>4.5</td>\n",
       "      <td>398307</td>\n",
       "      <td>19.0</td>\n",
       "      <td>10000000</td>\n",
       "      <td>0</td>\n",
       "      <td>iHoroscope - 2018 Daily Horoscope &amp; Astrology</td>\n",
       "      <td>LIFESTYLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9366 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rating  Reviews  Size  Installs  Price  \\\n",
       "0        4.1      159  19.0     10000      0   \n",
       "1        3.9      967  14.0    500000      0   \n",
       "2        4.7    87510   8.7   5000000      0   \n",
       "3        4.5   215644  25.0  50000000      0   \n",
       "4        4.3      967   2.8    100000      0   \n",
       "...      ...      ...   ...       ...    ...   \n",
       "9361     4.0        7   2.6       500      0   \n",
       "9362     4.5       38  53.0      5000      0   \n",
       "9363     5.0        4   3.6       100      0   \n",
       "9364     4.5      114   0.0      1000      0   \n",
       "9365     4.5   398307  19.0  10000000      0   \n",
       "\n",
       "                                                    App             Category  \n",
       "0        Photo Editor & Candy Camera & Grid & ScrapBook       ART_AND_DESIGN  \n",
       "1                                   Coloring book moana       ART_AND_DESIGN  \n",
       "2     U Launcher Lite – FREE Live Cool Themes, Hide ...       ART_AND_DESIGN  \n",
       "3                                 Sketch - Draw & Paint       ART_AND_DESIGN  \n",
       "4                 Pixel Draw - Number Art Coloring Book       ART_AND_DESIGN  \n",
       "...                                                 ...                  ...  \n",
       "9361                                      FR Calculator               FAMILY  \n",
       "9362                                   Sya9a Maroc - FR               FAMILY  \n",
       "9363                   Fr. Mike Schmitz Audio Teachings               FAMILY  \n",
       "9364                      The SCP Foundation DB fr nn5n  BOOKS_AND_REFERENCE  \n",
       "9365      iHoroscope - 2018 Daily Horoscope & Astrology            LIFESTYLE  \n",
       "\n",
       "[9366 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5 = pd.read_excel('data/part5_clustering/googleplaystore.xlsx', engine='openpyxl')\n",
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.1 :</p>\n",
    "\n",
    "Select the numerical features of the dataset then standardize them by removing the mean and scaling to unit variance. \n",
    "Then, do a first training using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm with $5$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.2 :</p>\n",
    "\n",
    "Complete the function `average_cluster_distance` to compute a clustering performance metric. It should return a list where the first value is the global average distance from all samples to their assigned centroids, followed by the average distances per centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cluster_distance(X, model):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "avg_distances = average_cluster_distance(...)\n",
    "print(avg_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.3 :</p>\n",
    "\n",
    "Try using different values for the maximum number of iterations of the *K-Means Algorithm*. Then, report your results and describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.4 :</p>\n",
    "\n",
    "Analyze how the category distributions change when clustering with $\\text{n\\_cluster} = {2,3,4,5}$ and compare the results. Observe how categories are grouped within each cluster and note any significant shifts as the number of clusters increases. Describe the distributions obtained and evaluate the average distances for each $\\text{n\\_cluster}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.4*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
