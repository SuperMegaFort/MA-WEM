{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Data Analysis - Web Mining\n",
    "\n",
    "The web is an immense source of data, offering valuable insights across various domains. In this assignment, we will explore different ways to process, and analyze web data to uncover meaningful patterns and make informed decisions. Through five practical exercises, we will apply key data mining techniques to real-world scenarios.\n",
    "\n",
    "Each exercise focuses on a specific application:\n",
    "\n",
    "* *Exercise 1* - Clickbait classification\n",
    "* *Exercise 2* - Sentiment analysis on comments\n",
    "* *Exercise 3* - Movie recommendation\n",
    "* *Exercise 4* - Association rules in online shopping\n",
    "* *Exercise 5* - Clustering of mobile apps\n",
    "\n",
    "These exercises will provide hands-on experience in working with real-world data collected from the web, helping you understand its potential for analysis.\n",
    "\n",
    "For this assignment, complete all exercises that are marked in <span style='color:red;font-weight:bold'>red</span>. Please make sure all your cells run correctly (try to *Clear All Outputs* then *Run All* once before submitting). **Check the cells outputs are visibles even for the coding parts**\n",
    "\n",
    "The assignment is due for <span style='color:red;font-weight:bold'>Thursday 27th of March 2025 at 23:59</span>.\n",
    "\n",
    "No report is needed as all questions can be answered directly in this notebook file. You only need to give this notebook file completed on the [Moodle assignment page](https://moodle.msengineering.ch/course/view.php?id=2732). Only one file per group is required for submission.\n",
    "\n",
    "If you have any questions or issues, please contact one of the assistants below:\n",
    "- C√©dric Campos Carvalho (*Teams* might be easier to discuss, mail: cedric.camposcarvalho@heig-vd.ch)\n",
    "- Elena Najdenovska (mail: elena.najdenovska@heig-vd.ch)\n",
    "\n",
    "\n",
    "Teacher : \n",
    "- Laura Elena Raileanu <Laura.Raileanu@heig-vd.ch>(mail: Laura.Raileanu@heig-vd.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Clickbait classification\n",
    "\n",
    "The objective of this first part is to model a filter for \"clickbait\" in online news media. Clickbait headlines are designed to attract attention and drive clicks, often at the expense of accuracy or relevance.\n",
    "\n",
    "To achieve this, we provide you with a dataset containing more than 10'000 press headlines collected in 2016. Each row in this dataset corresponds to a single headline, which is described by the following two attributes:\n",
    "\n",
    "* `headline`: the text representing the title\n",
    "* `clickbait`: the label identifying whether the title is a clickbait *(1)* or not *(0)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Need To Tell Us If These Things Are Doughnuts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15 Great Pieces Of Relationship Advice From Books</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improved E-Mail Service From a Dedicated Device</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two MBTA Green Line trains collide in Newton, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 Struggles All Smartypants Will Understand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10536</th>\n",
       "      <td>Can You Match The Phone To The R&amp;B Video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10537</th>\n",
       "      <td>19 Soul Food Recipes That Are Almost As Good A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10538</th>\n",
       "      <td>16 Photos Of Desis That Will Give You Intense ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539</th>\n",
       "      <td>City Plans to Make Older Buildings Refit to Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10540</th>\n",
       "      <td>Iraqis accept constitution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10541 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0      You Need To Tell Us If These Things Are Doughnuts          1\n",
       "1      15 Great Pieces Of Relationship Advice From Books          1\n",
       "2        Improved E-Mail Service From a Dedicated Device          0\n",
       "3      Two MBTA Green Line trains collide in Newton, ...          0\n",
       "4           17 Struggles All Smartypants Will Understand          1\n",
       "...                                                  ...        ...\n",
       "10536           Can You Match The Phone To The R&B Video          1\n",
       "10537  19 Soul Food Recipes That Are Almost As Good A...          1\n",
       "10538  16 Photos Of Desis That Will Give You Intense ...          1\n",
       "10539  City Plans to Make Older Buildings Refit to Sa...          0\n",
       "10540                         Iraqis accept constitution          0\n",
       "\n",
       "[10541 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_excel('data/part1_classification/news_clickbait.xlsx', engine=\"openpyxl\")\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.1 :</p>\n",
    "\n",
    "The first step is to separate the dataset into two sets (training and test), complete the input parameters of `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration des donn√©es en features (X) et cible (y)\n",
    "X = df_1['headline']\n",
    "y = df_1['clickbait']\n",
    "\n",
    "# S√©paration en ensembles d'entra√Ænement et de test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.2 :</p>\n",
    "\n",
    "Create a pre-processing `Pipeline` using [scikit-learn Pipeline object](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "In your pipeline you need:\n",
    "- Vectorize your headlines with Term frequency-inverse document frequency.\n",
    "- Transform your data in case of [`sparse matrix`](https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse), so the data goes through the model without any issues.\n",
    "\n",
    "**Do not forget to remove words giving no information for classification (i.e. [stop words](https://en.wikipedia.org/wiki/Stop_word)).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour convertir une matrice sparse en dense (n√©cessaire pour GaussianNB)\n",
    "def to_dense(X):\n",
    "    return X.toarray()\n",
    "\n",
    "# Pipeline de pr√©-traitement\n",
    "preprocessor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),  \n",
    "    ('densify', FunctionTransformer(to_dense, accept_sparse=True)), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.3 :</p>\n",
    "\n",
    "Create the pipeline using `make_pipeline`, combining the pre-processing pipeline and the `GaussianNB` model. Then, train the pipeline and find the score obtained with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (accuracy) sur l'ensemble de test : 0.89900426742532\n"
     ]
    }
   ],
   "source": [
    "# TODO 1.3\n",
    "# Cr√©ation du pipeline complet (pr√©-traitement + mod√®le)\n",
    "pipeline = make_pipeline(preprocessor, GaussianNB())\n",
    "\n",
    "# Entra√Ænement du pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# √âvaluation sur l'ensemble de test\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Score (accuracy) sur l'ensemble de test : {score}\")\n",
    "\n",
    "# Pr√©dictions sur l'ensemble de test (pour la suite)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.4 :</p>\n",
    "\n",
    "Modify your split ratio for the training/testing set and see if there's a difference in the model's performances. Please explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 0.1, Score: 0.8957345971563981\n",
      "Test size: 0.2, Score: 0.89900426742532\n",
      "Test size: 0.3, Score: 0.8921909579513121\n",
      "Test size: 0.4, Score: 0.8878349537585961\n"
     ]
    }
   ],
   "source": [
    "# Tester diff√©rents ratios de split\n",
    "for test_size in [0.1, 0.2, 0.3, 0.4]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print(f\"Test size: {test_size}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.5 :</p>\n",
    "\n",
    "Keep your split validation but now incorporate [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). Use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function and a K-Folding with 5 splits over the training set without using the test set. Then, calculate the averaged accuracy (with standard deviation) for 5 folds.\n",
    "\n",
    "**Explain the results obtained and how to read them compared to the split validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de validation crois√©e : [0.8743083  0.88458498 0.90671937 0.89249012 0.88370253]\n",
      "Moyenne des scores : 0.8883610596887979\n",
      "√âcart-type des scores : 0.010839901287151002\n"
     ]
    }
   ],
   "source": [
    "# TODO 1.5\n",
    "# Validation crois√©e avec 5 folds\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Scores de validation crois√©e : {cv_scores}\")\n",
    "print(f\"Moyenne des scores : {cv_scores.mean()}\")\n",
    "print(f\"√âcart-type des scores : {cv_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.6 :</p>\n",
    "\n",
    "Try to use atleast 3 different classifiers and report their results in a table. Compare the results between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.6\n",
    "# Dictionnaire des classifieurs\n",
    "classifiers = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "# Stocker les r√©sultats pour chaque classifieur\n",
    "results = {}\n",
    "\n",
    "# Boucle sur les classifieurs\n",
    "for name, clf in classifiers.items():\n",
    "    # Cr√©er le pipeline\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # Validation crois√©e\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Stocker les r√©sultats\n",
    "    results[name] = {\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_accuracy': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Afficher les r√©sultats sous forme de tableau\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.7 :</p>\n",
    "\n",
    "In text processing we often use a stemming step for the pre-processing, explain what it consists and how it can be useful then give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Sentiment analysis on comments\n",
    "\n",
    "The goal of the second part is to analyze tweets from the COVID-19 period and perform sentiment analysis to determine whether they express a positive or negative sentiment.\n",
    "\n",
    "In the first step, we will work with the dataset `CoronaTwitterComments_2labels.xlsx`, which contains approximately 3'000 comments related to COVID-19 from Twitter in March 2020. These comments are already labeled with *1* (positive comment) or *-1* (negative comment). In the second step, we will consider the dataset `CoronaTwitterComments_3labels.xlsx`, which includes additional neutral comments (labeled with *0*). You will see how we can process the text data with the help of [WordNet](https://wordnet.princeton.edu/) to retrieve a sentiment and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package sentiwordnet to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK packages needed for this exercise, feel free to add some if you need it.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [-1  1  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>SentimentLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>@RicePolitics @MDCounties Craig, will you call...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3179 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OriginalTweet  SentimentLabel\n",
       "0     TRENDING: New Yorkers encounter empty supermar...              -1\n",
       "1     When I couldn't find hand sanitizer at Fred Me...               1\n",
       "2     Find out how you can protect yourself and love...               1\n",
       "3     #Panic buying hits #NewYork City as anxious sh...              -1\n",
       "4     Voting in the age of #coronavirus = hand sanit...               1\n",
       "...                                                 ...             ...\n",
       "3174  @RicePolitics @MDCounties Craig, will you call...              -1\n",
       "3175  Meanwhile In A Supermarket in Israel -- People...               1\n",
       "3176  Did you panic buy a lot of non-perishable item...              -1\n",
       "3177  Gov need to do somethings instead of biar je r...              -1\n",
       "3178  I and @ForestandPaper members are committed to...               1\n",
       "\n",
       "[3179 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_tweets2 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_2labels.xlsx', engine=\"openpyxl\")\n",
    "corona_tweets3 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_3labels.xlsx', engine=\"openpyxl\")\n",
    "\n",
    "print(corona_tweets2.SentimentLabel.unique(), corona_tweets3.SentimentLabel.unique())\n",
    "corona_tweets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.1 :</p>\n",
    "\n",
    "The first step is to pre-process the text (like in previous exercise) to later use a WordNet sentiment analysis over it.\n",
    "\n",
    "The class inherits from `BaseEstimator` and `TransformerMixin`, ensuring seamless integration with other scikit-learn modules. This allows it to utilize the familiar `fit`, `transform`, and `fit_transform` functions of the library.\n",
    "\n",
    "Your task is to update the `NLTKPreprocessor` class with the needed functions to be used later in the `transform` function.\n",
    "- Add a tokenizer specialized in tweets.\n",
    "- Remove stop words and other unuseful characters.\n",
    "- Transform in lowercase the text\n",
    "- Lemmatize the text using for Wordnet.\n",
    "\n",
    "*Advice : If you encounter any issues, refer to the `apply_pipeline` function to understand how it works. This function should **not** be modified.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet, sentiwordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.pipeline = [\n",
    "            self.tokenize,\n",
    "            self.remove_stopwords,\n",
    "            self.lemmatize\n",
    "        ]\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
    "\n",
    "    def lemmatize(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.2 :</p>\n",
    "\n",
    "The next step is to create the `WordNetSentimentAnalyzer`. You will need to create three functions to add to your pipeline : \n",
    "1. The WordNet sentiment analysis requires to have the tag of each word of the sentence. Tagging in part of speech (POS) is the process of assigning grammatical categories, such as nouns, verbs, or adjectives, to words in a text based on their role in a sentence. `nltk` has a pre-trained model that can tag these words, find it and apply it to the pipeline.\n",
    "2. For WordNet the tagging is different from the pre-trained `nltk` model. Create a function replacing the tags with the WordNet tags using `nltk.corpus.wordnet` module.\n",
    "3. Create the sentiment function which sums the positive sentiment ($\\text{pos}$) and negative sentiment ($\\text{neg}$) of each word (using `nltk.corupus.sentiwordnet` module). The value needs to be taken in account only if it is superior to a certain threshold (i.e. $\\text{treshold} = 0.05$). Then, return a single value representing the sentiment of the sentence such as it is $\\text{sentiment} < 0$ if it's negative and $\\text{sentiment} > 0$ if positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetSentimentAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pipeline = [\n",
    "            self.pos_tagging,\n",
    "            self.convert_pos_tags,\n",
    "            self.compute_sentiment\n",
    "        ]\n",
    "\n",
    "    def pos_tagging(self, tokens):\n",
    "        return pos_tag(tokens)\n",
    "    \n",
    "    def convert_pos_tags(self, tagged_tokens):\n",
    "        tag_map = {\n",
    "            'J': wordnet.ADJ,\n",
    "            'V': wordnet.VERB,\n",
    "            'N': wordnet.NOUN,\n",
    "            'R': wordnet.ADV\n",
    "        }\n",
    "        return [(word, tag_map.get(tag[0], wordnet.NOUN)) for word, tag in tagged_tokens]\n",
    "    \n",
    "    def compute_sentiment(self, tagged_tokens):\n",
    "        sentiment = 0\n",
    "        threshold = 0.05\n",
    "        \n",
    "        for word, tag in tagged_tokens:\n",
    "            synsets = list(sentiwordnet.senti_synsets(word, tag))\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                pos, neg = synset.pos_score(), synset.neg_score()\n",
    "                if max(pos, neg) > threshold:\n",
    "                    sentiment += pos - neg\n",
    "        return sentiment\n",
    "\n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.3 :</p>\n",
    "\n",
    "Create two functions that return the sentiment class to compare it to the labels.\n",
    "* `get_sentiment_class_2` : For positive and negative only.\n",
    "* `get_sentiment_class_3` : For positive, negative and neutral.\n",
    "\n",
    "**Do not forget that it is going to be used in a `scikit-learn` `Pipeline`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_class_2(sent_val):\n",
    "    return 1 if sent_val > 0 else -1\n",
    "\n",
    "def get_sentiment_class_3(sent_val):\n",
    "    if sent_val > 0:\n",
    "        return 1\n",
    "    elif sent_val < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.4 :</p>\n",
    "\n",
    "Create a `scikit-learn` `Pipeline` using the 3 previous *Transformers* that you created (using the 2 class sentiment).\n",
    "\n",
    "Then, complete the function `get_sentiment_tweet` and test it with your own Tweet (**Do not change its signature !**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', NLTKPreprocessor()),\n",
    "    ('sentiment_analyzer', WordNetSentimentAnalyzer())\n",
    "])\n",
    "\n",
    "def get_sentiment_tweet(tweet:str)-> int:\n",
    "    sentiment = pipeline.transform([tweet])[0]\n",
    "    return get_sentiment_class_2(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.5 :</p>\n",
    "\n",
    "Compute the accuracy obtained with all tweets of the dataset. Then, do the same for the 3 labels class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2-label classification: 0.6517772884554891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 3-label classification: 0.545550289626119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline.fit(corona_tweets2['OriginalTweet'], corona_tweets2['SentimentLabel'])\n",
    "\n",
    "y_true_2 = corona_tweets2['SentimentLabel']\n",
    "y_pred_2 = [get_sentiment_tweet(tweet) for tweet in corona_tweets2['OriginalTweet']]\n",
    "accuracy_2 = accuracy_score(y_true_2, y_pred_2)\n",
    "print(f\"Accuracy for 2-label classification: {accuracy_2}\")\n",
    "\n",
    "pipeline.fit(corona_tweets3['OriginalTweet'], corona_tweets3['SentimentLabel'])\n",
    "\n",
    "y_true_3 = corona_tweets3['SentimentLabel']\n",
    "y_pred_3 = [get_sentiment_tweet(tweet) for tweet in corona_tweets3['OriginalTweet']]\n",
    "accuracy_3 = accuracy_score(y_true_3, y_pred_3)\n",
    "print(f\"Accuracy for 3-label classification: {accuracy_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.6 :</p>\n",
    "\n",
    "Analyze the results of each pre-processing (`NLTKPreprocessor`) step to try to understand what you could do to improve it. For example, what could you add into this pipeline ? *(No implementation needed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline can be improved by better handling emojis and hashtags, as they often carry sentiment (e.g., üòÄ ‚Üí \"happy\"). \n",
    "Removing URLs, mentions (`@user`), and refining stopwords with a domain-specific list would reduce noise. Enhancing POS tagging with a more advanced model (e.g., spaCy) would improve lemmatization. \n",
    "A spell checker like SymSpell could correct typos common in tweets. Additionally, considering negations (e.g., \"not good\" should be negative) would refine sentiment detection. \n",
    "These improvements would likely enhance classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Movie recommendation\n",
    "\n",
    "The objective of this exercise is to create a recommendation system of movies based on the user ratings. We will focus on the collaborative approach for movie recommendation using the provided dataset, which contains approximately 9'800 movies rated by 610 users from MovieLens.\n",
    "\n",
    "More specifically, the `movies.csv` dataset, which describes the movies, includes three attributes:\n",
    "\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `title`: the title of the movie (with the release year in parentheses)\n",
    "* `genres`: the genres of the movie\n",
    "\n",
    "The `ratings.csv` dataset, which contains user ratings for the movies, includes four attributes:\n",
    "\n",
    "* `userId`: unique identifier of the user\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `rating`: the user's rating for the corresponding movie\n",
    "* `timestamp`: the timestamp of the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "      <th>userId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>964982703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>847434962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>7</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1106635946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>15</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1510577970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy</td>\n",
       "      <td>17</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1305696483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100831</th>\n",
       "      <td>193581</td>\n",
       "      <td>Black Butler: Book of the Atlantic (2017)</td>\n",
       "      <td>Action|Animation|Comedy|Fantasy</td>\n",
       "      <td>184</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1537109082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100832</th>\n",
       "      <td>193583</td>\n",
       "      <td>No Game No Life: Zero (2017)</td>\n",
       "      <td>Animation|Comedy|Fantasy</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537109545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100833</th>\n",
       "      <td>193585</td>\n",
       "      <td>Flint (2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537109805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100834</th>\n",
       "      <td>193587</td>\n",
       "      <td>Bungo Stray Dogs: Dead Apple (2018)</td>\n",
       "      <td>Action|Animation</td>\n",
       "      <td>184</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1537110021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100835</th>\n",
       "      <td>193609</td>\n",
       "      <td>Andrew Dice Clay: Dice Rules (1991)</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1537157606</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100836 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        movieId                                      title  \\\n",
       "0             1                           Toy Story (1995)   \n",
       "1             1                           Toy Story (1995)   \n",
       "2             1                           Toy Story (1995)   \n",
       "3             1                           Toy Story (1995)   \n",
       "4             1                           Toy Story (1995)   \n",
       "...         ...                                        ...   \n",
       "100831   193581  Black Butler: Book of the Atlantic (2017)   \n",
       "100832   193583               No Game No Life: Zero (2017)   \n",
       "100833   193585                               Flint (2017)   \n",
       "100834   193587        Bungo Stray Dogs: Dead Apple (2018)   \n",
       "100835   193609        Andrew Dice Clay: Dice Rules (1991)   \n",
       "\n",
       "                                             genres  userId  rating  \\\n",
       "0       Adventure|Animation|Children|Comedy|Fantasy       1     4.0   \n",
       "1       Adventure|Animation|Children|Comedy|Fantasy       5     4.0   \n",
       "2       Adventure|Animation|Children|Comedy|Fantasy       7     4.5   \n",
       "3       Adventure|Animation|Children|Comedy|Fantasy      15     2.5   \n",
       "4       Adventure|Animation|Children|Comedy|Fantasy      17     4.5   \n",
       "...                                             ...     ...     ...   \n",
       "100831              Action|Animation|Comedy|Fantasy     184     4.0   \n",
       "100832                     Animation|Comedy|Fantasy     184     3.5   \n",
       "100833                                        Drama     184     3.5   \n",
       "100834                             Action|Animation     184     3.5   \n",
       "100835                                       Comedy     331     4.0   \n",
       "\n",
       "         timestamp  \n",
       "0        964982703  \n",
       "1        847434962  \n",
       "2       1106635946  \n",
       "3       1510577970  \n",
       "4       1305696483  \n",
       "...            ...  \n",
       "100831  1537109082  \n",
       "100832  1537109545  \n",
       "100833  1537109805  \n",
       "100834  1537110021  \n",
       "100835  1537157606  \n",
       "\n",
       "[100836 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies = pd.read_csv('data/part3_recommandationdesfilms/movies.csv')\n",
    "df_ratings = pd.read_csv('data/part3_recommandationdesfilms/ratings.csv')\n",
    "\n",
    "df_3 = pd.merge(df_movies, df_ratings, on='movieId')\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.1 :</p>\n",
    "\n",
    "Use `df_3` to create a second `DataFrame` with the rating of every movies for each `userId`. If the user never watched a movie the value is $0$. \n",
    "For this step, use the [`pivot` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4. , 0. , 4. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       ...,\n",
       "       [2.5, 2. , 2. , ..., 0. , 0. , 0. ],\n",
       "       [3. , 0. , 0. , ..., 0. , 0. , 0. ],\n",
       "       [5. , 0. , 0. , ..., 0. , 0. , 0. ]], shape=(610, 9724))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_item_matrix = df_3.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "X = user_item_matrix.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.2 :</p>\n",
    "\n",
    "First separate the data using `train_test_split` function with $20\\%$ of the data in the test set.\n",
    "Create an `knn` model using the [`NearestNeighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) model from `scikit-learn`. For now use $N=80$, and the `cosine` metric, then train it.\n",
    "\n",
    "**Explain in few sentences how does the `cosine` metric works by taking our case in an example (include the formula).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"‚ñ∏\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"‚ñæ\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=80)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>NearestNeighbors</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.neighbors.NearestNeighbors.html\">?<span>Documentation for NearestNeighbors</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>NearestNeighbors(metric=&#x27;cosine&#x27;, n_neighbors=80)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric='cosine', n_neighbors=80)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)\n",
    "\n",
    "knn = NearestNeighbors(n_neighbors=80, metric='cosine')\n",
    "knn.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cosine similarity is calculated as: <br>\n",
    "$$ \\cos(\\theta) = \\frac{(A \\cdot B)}{(||A|| * ||B||)} $$\n",
    "In our case, A and B are two user vectors, and the closer the cosine similarity is to 1,\n",
    "the more similar the users are in terms of their movie preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.3 :</p>\n",
    "\n",
    "Create the `predict` function by using the previous trained model.\n",
    "\n",
    "*Hints : Think about the model returns and what they represent. Then, use the closest indexes to obtain information on the best matching movies.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, user_item_matrix, knn):\n",
    "    distances, indices = knn.kneighbors(X)\n",
    "    predictions = []\n",
    "    \n",
    "    for i, neighbors in enumerate(indices):\n",
    "        neighbor_ratings = user_item_matrix.iloc[neighbors].mean(axis=0)\n",
    "        predictions.append(neighbor_ratings)\n",
    "    \n",
    "    return np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.4 :</p>\n",
    "\n",
    "Finally, complete the `prec_at_k` function, which measures the precision of our model. This metric calculates how many of the top $k$ movies recommended to a user are actually relevant. A movie is considered relevant if the user has given it a score of at least 4.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_at_k(Y_pred, user_item_matrix, k):\n",
    "    precision_scores = []\n",
    "    \n",
    "    for i in range(Y_pred.shape[0]):\n",
    "        top_k_predictions = np.argsort(Y_pred[i])[-k:]\n",
    "        relevant_movies = set(np.where(user_item_matrix.iloc[i] >= 4)[0])\n",
    "        recommended_movies = set(top_k_predictions)\n",
    "        precision = len(recommended_movies & relevant_movies) / k\n",
    "        precision_scores.append(precision)\n",
    "    \n",
    "    return np.mean(precision_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.5 :</p>\n",
    "\n",
    "Now, use the function with three different values for k ($5$, $15$, $25$) each time. Then, use different $N$ neighbours and a different split ratio.\n",
    "\n",
    "Report all your results bellow and explain the impact of these hyperparameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision at 5 with 50 neighbors: 0.3705\n",
      "Precision at 5 with 100 neighbors: 0.4033\n",
      "Precision at 5 with 150 neighbors: 0.4082\n",
      "Precision at 15 with 50 neighbors: 0.3044\n",
      "Precision at 15 with 100 neighbors: 0.3131\n",
      "Precision at 15 with 150 neighbors: 0.3093\n",
      "Precision at 25 with 50 neighbors: 0.2675\n",
      "Precision at 25 with 100 neighbors: 0.2744\n",
      "Precision at 25 with 150 neighbors: 0.2777\n"
     ]
    }
   ],
   "source": [
    "for k in [5, 15, 25]:\n",
    "    for N in [50, 100, 150]:\n",
    "        knn = NearestNeighbors(n_neighbors=N, metric='cosine')\n",
    "        knn.fit(X_train)\n",
    "        Y_pred = predict(X_test, user_item_matrix, knn)\n",
    "        precision = prec_at_k(Y_pred, user_item_matrix, k)\n",
    "        print(f'Precision at {k} with {N} neighbors: {precision:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Hyperparameters on Precision\n",
    "\n",
    "- **Number of Neighbors (N)**: Increasing the number of neighbors from 50 to 150 leads to a slight improvement in precision at k = 5. However, the improvement is minimal, suggesting that larger values of N may not significantly boost the model's performance. The precision stabilizes around 0.40 for higher neighbor values.\n",
    "\n",
    "- **k Value**: As we increase k (from 5 to 25), the precision decreases. This indicates that recommending more movies dilutes the relevance of the top recommendations. With larger k values, the model likely includes less relevant recommendations, thus lowering precision.\n",
    "\n",
    "### Conclusion:\n",
    "The optimal number of neighbors seems to be around 100, and smaller k values (like 5) perform better in terms of precision. Increasing k or N beyond a certain point offers diminishing returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Association rules in online shopping\n",
    "\n",
    "In this fourth part, we will focus on a **Market Basket Analysis** problem. The dataset provided contains **online sales transactions** from an e-commerce site over one year. Our goal is to generate **association rules** based on these sales to identify which items are frequently bought together. You can refer to the source mentioned in the **README file** included with the data for more details.  \n",
    "\n",
    "The dataset is structured as follows, with each row representing the details of a product sale:  \n",
    "- `InvoiceNo`: invoice/sale identifier  \n",
    "- `StockCode`: product identifier  \n",
    "- `Description`: purchased product  \n",
    "- `Quantity`: quantity sold  \n",
    "- `InvoiceDate`: order/payment date  \n",
    "- `UnitPrice`: price of the product  \n",
    "- `CustomerID`: customer identifier  \n",
    "- `Country`: customer‚Äôs country of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_excel('data/part4_marketbasketanalysis/OnlineRetailDataset.xlsx', engine='openpyxl')\n",
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.1 :</p>\n",
    "\n",
    "Transform the given dataset into a one-hot encoded format for association rule mining. Group items by customer so that each row represents a unique customer with a list of purchased items. Then, convert this into a binary matrix where each column is an item, and values indicate whether a customer bought that item (1) or not (0). Use [`MultiLabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.2 :</p>\n",
    "\n",
    "Finally, create the association rules based on the frequent patterns obtained through `FP-growth` algorithm. Check [`mlxtend`](https://rasbt.github.io/mlxtend/) documentation to complete this task. Then, show what are the association rules with a minimum of $0.9$ confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.3 :</p>\n",
    "\n",
    "Do you observe any changes in the different parameters of the *FP-Growth* and *Association Rules* functions ? Please comment on the chosen parameters and the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.4 :</p>\n",
    "\n",
    "Is it possible to use other column(s) from the initial data to generate interesting rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Clustering of mobile apps\n",
    "\n",
    "In this final part, we will perform clustering of applications from the Google Play Store. You can refer to the source mentioned in the **README file** included with the data for more details.\n",
    "\n",
    "Among the attributes available in the `googleplaystore.xlsx` file, we will use the following:\n",
    "\n",
    "* `Rating`: overall user rating of the application\n",
    "* `Reviews`: number of user reviews\n",
    "* `Size`: size of the application\n",
    "* `Installs`: number of users who installed the application\n",
    "* `Price`: price of the application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.read_excel('data/part5_clustering/googleplaystore.xlsx', engine='openpyxl')\n",
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.1 :</p>\n",
    "\n",
    "Select the numerical features of the dataset then standardize them by removing the mean and scaling to unit variance. \n",
    "Then, do a first training using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm with $5$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.2 :</p>\n",
    "\n",
    "Complete the function `average_cluster_distance` to compute a clustering performance metric. It should return a list where the first value is the global average distance from all samples to their assigned centroids, followed by the average distances per centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cluster_distance(X, model):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "avg_distances = average_cluster_distance(...)\n",
    "print(avg_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.3 :</p>\n",
    "\n",
    "Try using different values for the maximum number of iterations of the *K-Means Algorithm*. Then, report your results and describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.4 :</p>\n",
    "\n",
    "Analyze how the category distributions change when clustering with $\\text{n\\_cluster} = {2,3,4,5}$ and compare the results. Observe how categories are grouped within each cluster and note any significant shifts as the number of clusters increases. Describe the distributions obtained and evaluate the average distances for each $\\text{n\\_cluster}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.4*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
