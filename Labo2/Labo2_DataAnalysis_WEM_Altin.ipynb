{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2 - Data Analysis - Web Mining\n",
    "\n",
    "The web is an immense source of data, offering valuable insights across various domains. In this assignment, we will explore different ways to process, and analyze web data to uncover meaningful patterns and make informed decisions. Through five practical exercises, we will apply key data mining techniques to real-world scenarios.\n",
    "\n",
    "Each exercise focuses on a specific application:\n",
    "\n",
    "* *Exercise 1* - Clickbait classification\n",
    "* *Exercise 2* - Sentiment analysis on comments\n",
    "* *Exercise 3* - Movie recommendation\n",
    "* *Exercise 4* - Association rules in online shopping\n",
    "* *Exercise 5* - Clustering of mobile apps\n",
    "\n",
    "These exercises will provide hands-on experience in working with real-world data collected from the web, helping you understand its potential for analysis.\n",
    "\n",
    "For this assignment, complete all exercises that are marked in <span style='color:red;font-weight:bold'>red</span>. Please make sure all your cells run correctly (try to *Clear All Outputs* then *Run All* once before submitting). **Check the cells outputs are visibles even for the coding parts**\n",
    "\n",
    "The assignment is due for <span style='color:red;font-weight:bold'>Thursday 27th of March 2025 at 23:59</span>.\n",
    "\n",
    "No report is needed as all questions can be answered directly in this notebook file. You only need to give this notebook file completed on the [Moodle assignment page](https://moodle.msengineering.ch/course/view.php?id=2732). Only one file per group is required for submission.\n",
    "\n",
    "If you have any questions or issues, please contact one of the assistants below:\n",
    "- Cédric Campos Carvalho (*Teams* might be easier to discuss, mail: cedric.camposcarvalho@heig-vd.ch)\n",
    "- Elena Najdenovska (mail: elena.najdenovska@heig-vd.ch)\n",
    "\n",
    "\n",
    "Teacher : \n",
    "- Laura Elena Raileanu <Laura.Raileanu@heig-vd.ch>(mail: Laura.Raileanu@heig-vd.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 - Clickbait classification\n",
    "\n",
    "The objective of this first part is to model a filter for \"clickbait\" in online news media. Clickbait headlines are designed to attract attention and drive clicks, often at the expense of accuracy or relevance.\n",
    "\n",
    "To achieve this, we provide you with a dataset containing more than 10'000 press headlines collected in 2016. Each row in this dataset corresponds to a single headline, which is described by the following two attributes:\n",
    "\n",
    "* `headline`: the text representing the title\n",
    "* `clickbait`: the label identifying whether the title is a clickbait *(1)* or not *(0)*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>clickbait</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You Need To Tell Us If These Things Are Doughnuts</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15 Great Pieces Of Relationship Advice From Books</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Improved E-Mail Service From a Dedicated Device</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Two MBTA Green Line trains collide in Newton, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 Struggles All Smartypants Will Understand</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10536</th>\n",
       "      <td>Can You Match The Phone To The R&amp;B Video</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10537</th>\n",
       "      <td>19 Soul Food Recipes That Are Almost As Good A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10538</th>\n",
       "      <td>16 Photos Of Desis That Will Give You Intense ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10539</th>\n",
       "      <td>City Plans to Make Older Buildings Refit to Sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10540</th>\n",
       "      <td>Iraqis accept constitution</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10541 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  clickbait\n",
       "0      You Need To Tell Us If These Things Are Doughnuts          1\n",
       "1      15 Great Pieces Of Relationship Advice From Books          1\n",
       "2        Improved E-Mail Service From a Dedicated Device          0\n",
       "3      Two MBTA Green Line trains collide in Newton, ...          0\n",
       "4           17 Struggles All Smartypants Will Understand          1\n",
       "...                                                  ...        ...\n",
       "10536           Can You Match The Phone To The R&B Video          1\n",
       "10537  19 Soul Food Recipes That Are Almost As Good A...          1\n",
       "10538  16 Photos Of Desis That Will Give You Intense ...          1\n",
       "10539  City Plans to Make Older Buildings Refit to Sa...          0\n",
       "10540                         Iraqis accept constitution          0\n",
       "\n",
       "[10541 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = pd.read_excel('data/part1_classification/news_clickbait.xlsx', engine=\"openpyxl\")\n",
    "df_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.1 :</p>\n",
    "\n",
    "The first step is to separate the dataset into two sets (training and test), complete the input parameters of `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation des données en features (X) et cible (y)\n",
    "X = df_1['headline']\n",
    "y = df_1['clickbait']\n",
    "\n",
    "# Séparation en ensembles d'entraînement et de test (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.2 :</p>\n",
    "\n",
    "Create a pre-processing `Pipeline` using [scikit-learn Pipeline object](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
    "In your pipeline you need:\n",
    "- Vectorize your headlines with Term frequency-inverse document frequency.\n",
    "- Transform your data in case of [`sparse matrix`](https://docs.scipy.org/doc/scipy/reference/sparse.html#module-scipy.sparse), so the data goes through the model without any issues.\n",
    "\n",
    "**Do not forget to remove words giving no information for classification (i.e. [stop words](https://en.wikipedia.org/wiki/Stop_word)).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour convertir une matrice sparse en dense (nécessaire pour GaussianNB)\n",
    "def to_dense(X):\n",
    "    return X.toarray()\n",
    "\n",
    "# Pipeline de pré-traitement\n",
    "preprocessor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),  \n",
    "    ('densify', FunctionTransformer(to_dense, accept_sparse=True)), \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.3 :</p>\n",
    "\n",
    "Create the pipeline using `make_pipeline`, combining the pre-processing pipeline and the `GaussianNB` model. Then, train the pipeline and find the score obtained with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score (accuracy) sur l'ensemble de test : 0.89900426742532\n"
     ]
    }
   ],
   "source": [
    "# TODO 1.3\n",
    "# Création du pipeline complet (pré-traitement + modèle)\n",
    "pipeline = make_pipeline(preprocessor, GaussianNB())\n",
    "\n",
    "# Entraînement du pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Évaluation sur l'ensemble de test\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Score (accuracy) sur l'ensemble de test : {score}\")\n",
    "\n",
    "# Prédictions sur l'ensemble de test (pour la suite)\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.4 :</p>\n",
    "\n",
    "Modify your split ratio for the training/testing set and see if there's a difference in the model's performances. Please explain your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.4*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 0.1, Score: 0.8957345971563981\n",
      "Test size: 0.2, Score: 0.89900426742532\n",
      "Test size: 0.3, Score: 0.8921909579513121\n",
      "Test size: 0.4, Score: 0.8878349537585961\n"
     ]
    }
   ],
   "source": [
    "# Tester différents ratios de split\n",
    "for test_size in [0.1, 0.2, 0.3, 0.4]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    print(f\"Test size: {test_size}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.5 :</p>\n",
    "\n",
    "Keep your split validation but now incorporate [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)). Use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function and a K-Folding with 5 splits over the training set without using the test set. Then, calculate the averaged accuracy (with standard deviation) for 5 folds.\n",
    "\n",
    "**Explain the results obtained and how to read them compared to the split validation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores de validation croisée : [0.8743083  0.88458498 0.90671937 0.89249012 0.88370253]\n",
      "Moyenne des scores : 0.8883610596887979\n",
      "Écart-type des scores : 0.010839901287151002\n"
     ]
    }
   ],
   "source": [
    "# TODO 1.5\n",
    "# Validation croisée avec 5 folds\n",
    "cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"Scores de validation croisée : {cv_scores}\")\n",
    "print(f\"Moyenne des scores : {cv_scores.mean()}\")\n",
    "print(f\"Écart-type des scores : {cv_scores.std()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.6 :</p>\n",
    "\n",
    "Try to use atleast 3 different classifiers and report their results in a table. Compare the results between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1.6\n",
    "# Dictionnaire des classifieurs\n",
    "classifiers = {\n",
    "    'GaussianNB': GaussianNB(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'SVC': SVC()\n",
    "}\n",
    "\n",
    "# Stocker les résultats pour chaque classifieur\n",
    "results = {}\n",
    "\n",
    "# Boucle sur les classifieurs\n",
    "for name, clf in classifiers.items():\n",
    "    # Créer le pipeline\n",
    "    pipeline = make_pipeline(preprocessor, clf)\n",
    "\n",
    "    # Validation croisée\n",
    "    cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Stocker les résultats\n",
    "    results[name] = {\n",
    "        'mean_accuracy': cv_scores.mean(),\n",
    "        'std_accuracy': cv_scores.std()\n",
    "    }\n",
    "\n",
    "# Afficher les résultats sous forme de tableau\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 1.7 :</p>\n",
    "\n",
    "In text processing we often use a stemming step for the pre-processing, explain what it consists and how it can be useful then give an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 1.7*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2 - Sentiment analysis on comments\n",
    "\n",
    "The goal of the second part is to analyze tweets from the COVID-19 period and perform sentiment analysis to determine whether they express a positive or negative sentiment.\n",
    "\n",
    "In the first step, we will work with the dataset `CoronaTwitterComments_2labels.xlsx`, which contains approximately 3'000 comments related to COVID-19 from Twitter in March 2020. These comments are already labeled with *1* (positive comment) or *-1* (negative comment). In the second step, we will consider the dataset `CoronaTwitterComments_3labels.xlsx`, which includes additional neutral comments (labeled with *0*). You will see how we can process the text data with the help of [WordNet](https://wordnet.princeton.edu/) to retrieve a sentiment and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\nltk_data...\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package sentiwordnet to C:\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\sentiwordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK packages needed for this exercise, feel free to add some if you need it.\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [-1  1  0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\openpyxl\\styles\\stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>SentimentLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRENDING: New Yorkers encounter empty supermar...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>When I couldn't find hand sanitizer at Fred Me...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Find out how you can protect yourself and love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Panic buying hits #NewYork City as anxious sh...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Voting in the age of #coronavirus = hand sanit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>@RicePolitics @MDCounties Craig, will you call...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3175</th>\n",
       "      <td>Meanwhile In A Supermarket in Israel -- People...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>Did you panic buy a lot of non-perishable item...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3177</th>\n",
       "      <td>Gov need to do somethings instead of biar je r...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3178</th>\n",
       "      <td>I and @ForestandPaper members are committed to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3179 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          OriginalTweet  SentimentLabel\n",
       "0     TRENDING: New Yorkers encounter empty supermar...              -1\n",
       "1     When I couldn't find hand sanitizer at Fred Me...               1\n",
       "2     Find out how you can protect yourself and love...               1\n",
       "3     #Panic buying hits #NewYork City as anxious sh...              -1\n",
       "4     Voting in the age of #coronavirus = hand sanit...               1\n",
       "...                                                 ...             ...\n",
       "3174  @RicePolitics @MDCounties Craig, will you call...              -1\n",
       "3175  Meanwhile In A Supermarket in Israel -- People...               1\n",
       "3176  Did you panic buy a lot of non-perishable item...              -1\n",
       "3177  Gov need to do somethings instead of biar je r...              -1\n",
       "3178  I and @ForestandPaper members are committed to...               1\n",
       "\n",
       "[3179 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_tweets2 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_2labels.xlsx', engine=\"openpyxl\")\n",
    "corona_tweets3 = pd.read_excel('data/part2_sentimentanalysis/CoronaTwitterComments_3labels.xlsx', engine=\"openpyxl\")\n",
    "\n",
    "print(corona_tweets2.SentimentLabel.unique(), corona_tweets3.SentimentLabel.unique())\n",
    "corona_tweets2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.1 :</p>\n",
    "\n",
    "The first step is to pre-process the text (like in previous exercise) to later use a WordNet sentiment analysis over it.\n",
    "\n",
    "The class inherits from `BaseEstimator` and `TransformerMixin`, ensuring seamless integration with other scikit-learn modules. This allows it to utilize the familiar `fit`, `transform`, and `fit_transform` functions of the library.\n",
    "\n",
    "Your task is to update the `NLTKPreprocessor` class with the needed functions to be used later in the `transform` function.\n",
    "- Add a tokenizer specialized in tweets.\n",
    "- Remove stop words and other unuseful characters.\n",
    "- Transform in lowercase the text\n",
    "- Lemmatize the text using for Wordnet.\n",
    "\n",
    "*Advice : If you encounter any issues, refer to the `apply_pipeline` function to understand how it works. This function should **not** be modified.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords, wordnet, sentiwordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "class NLTKPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.tokenizer = TweetTokenizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.pipeline = [\n",
    "            self.tokenize,\n",
    "            self.remove_stopwords,\n",
    "            self.lemmatize\n",
    "        ]\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        return self.tokenizer.tokenize(text.lower())\n",
    "    \n",
    "    def remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word.isalpha() and word not in self.stop_words]\n",
    "\n",
    "    def lemmatize(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in tokens]\n",
    "        \n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.2 :</p>\n",
    "\n",
    "The next step is to create the `WordNetSentimentAnalyzer`. You will need to create three functions to add to your pipeline : \n",
    "1. The WordNet sentiment analysis requires to have the tag of each word of the sentence. Tagging in part of speech (POS) is the process of assigning grammatical categories, such as nouns, verbs, or adjectives, to words in a text based on their role in a sentence. `nltk` has a pre-trained model that can tag these words, find it and apply it to the pipeline.\n",
    "2. For WordNet the tagging is different from the pre-trained `nltk` model. Create a function replacing the tags with the WordNet tags using `nltk.corpus.wordnet` module.\n",
    "3. Create the sentiment function which sums the positive sentiment ($\\text{pos}$) and negative sentiment ($\\text{neg}$) of each word (using `nltk.corupus.sentiwordnet` module). The value needs to be taken in account only if it is superior to a certain threshold (i.e. $\\text{treshold} = 0.05$). Then, return a single value representing the sentiment of the sentence such as it is $\\text{sentiment} < 0$ if it's negative and $\\text{sentiment} > 0$ if positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordNetSentimentAnalyzer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.pipeline = [\n",
    "            self.pos_tagging,\n",
    "            self.convert_pos_tags,\n",
    "            self.compute_sentiment\n",
    "        ]\n",
    "\n",
    "    def pos_tagging(self, tokens):\n",
    "        return pos_tag(tokens)\n",
    "    \n",
    "    def convert_pos_tags(self, tagged_tokens):\n",
    "        tag_map = {\n",
    "            'J': wordnet.ADJ,\n",
    "            'V': wordnet.VERB,\n",
    "            'N': wordnet.NOUN,\n",
    "            'R': wordnet.ADV\n",
    "        }\n",
    "        return [(word, tag_map.get(tag[0], wordnet.NOUN)) for word, tag in tagged_tokens]\n",
    "    \n",
    "    def compute_sentiment(self, tagged_tokens):\n",
    "        sentiment = 0\n",
    "        threshold = 0.05\n",
    "        \n",
    "        for word, tag in tagged_tokens:\n",
    "            synsets = list(sentiwordnet.senti_synsets(word, tag))\n",
    "            if synsets:\n",
    "                synset = synsets[0]\n",
    "                pos, neg = synset.pos_score(), synset.neg_score()\n",
    "                if max(pos, neg) > threshold:\n",
    "                    sentiment += pos - neg\n",
    "        return sentiment\n",
    "\n",
    "    def apply_pipeline(self, x):\n",
    "        for transform in self.pipeline:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return [self.apply_pipeline(x) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.3 :</p>\n",
    "\n",
    "Create two functions that return the sentiment class to compare it to the labels.\n",
    "* `get_sentiment_class_2` : For positive and negative only.\n",
    "* `get_sentiment_class_3` : For positive, negative and neutral.\n",
    "\n",
    "**Do not forget that it is going to be used in a `scikit-learn` `Pipeline`!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_class_2(sent_val):\n",
    "    return 1 if sent_val > 0 else -1\n",
    "\n",
    "def get_sentiment_class_3(sent_val):\n",
    "    if sent_val > 0:\n",
    "        return 1\n",
    "    elif sent_val < 0:\n",
    "        return -1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.4 :</p>\n",
    "\n",
    "Create a `scikit-learn` `Pipeline` using the 3 previous *Transformers* that you created (using the 2 class sentiment).\n",
    "\n",
    "Then, complete the function `get_sentiment_tweet` and test it with your own Tweet (**Do not change its signature !**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', NLTKPreprocessor()),\n",
    "    ('sentiment_analyzer', WordNetSentimentAnalyzer())\n",
    "])\n",
    "\n",
    "def get_sentiment_tweet(tweet:str)-> int:\n",
    "    sentiment = pipeline.transform([tweet])[0]\n",
    "    return get_sentiment_class_2(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.5 :</p>\n",
    "\n",
    "Compute the accuracy obtained with all tweets of the dataset. Then, do the same for the 3 labels class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 2-label classification: 0.6517772884554891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\HES-SO\\MA-WEM\\Labos\\.venv\\lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 3-label classification: 0.545550289626119\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pipeline.fit(corona_tweets2['OriginalTweet'], corona_tweets2['SentimentLabel'])\n",
    "\n",
    "y_true_2 = corona_tweets2['SentimentLabel']\n",
    "y_pred_2 = [get_sentiment_tweet(tweet) for tweet in corona_tweets2['OriginalTweet']]\n",
    "accuracy_2 = accuracy_score(y_true_2, y_pred_2)\n",
    "print(f\"Accuracy for 2-label classification: {accuracy_2}\")\n",
    "\n",
    "pipeline.fit(corona_tweets3['OriginalTweet'], corona_tweets3['SentimentLabel'])\n",
    "\n",
    "y_true_3 = corona_tweets3['SentimentLabel']\n",
    "y_pred_3 = [get_sentiment_tweet(tweet) for tweet in corona_tweets3['OriginalTweet']]\n",
    "accuracy_3 = accuracy_score(y_true_3, y_pred_3)\n",
    "print(f\"Accuracy for 3-label classification: {accuracy_3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 2.6 :</p>\n",
    "\n",
    "Analyze the results of each pre-processing (`NLTKPreprocessor`) step to try to understand what you could do to improve it. For example, what could you add into this pipeline ? *(No implementation needed)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing pipeline can be improved by better handling emojis and hashtags, as they often carry sentiment (e.g., 😀 → \"happy\"). \n",
    "Removing URLs, mentions (`@user`), and refining stopwords with a domain-specific list would reduce noise. Enhancing POS tagging with a more advanced model (e.g., spaCy) would improve lemmatization. \n",
    "A spell checker like SymSpell could correct typos common in tweets. Additionally, considering negations (e.g., \"not good\" should be negative) would refine sentiment detection. \n",
    "These improvements would likely enhance classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 - Movie recommendation\n",
    "\n",
    "The objective of this exercise is to create a recommendation system of movies based on the user ratings. We will focus on the collaborative approach for movie recommendation using the provided dataset, which contains approximately 9'800 movies rated by 610 users from MovieLens.\n",
    "\n",
    "More specifically, the `movies.csv` dataset, which describes the movies, includes three attributes:\n",
    "\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `title`: the title of the movie (with the release year in parentheses)\n",
    "* `genres`: the genres of the movie\n",
    "\n",
    "The `ratings.csv` dataset, which contains user ratings for the movies, includes four attributes:\n",
    "\n",
    "* `userId`: unique identifier of the user\n",
    "* `movieId`: unique identifier of the movie\n",
    "* `rating`: the user's rating for the corresponding movie\n",
    "* `timestamp`: the timestamp of the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('data/part3_recommandationdesfilms/movies.csv')\n",
    "df_ratings = pd.read_csv('data/part3_recommandationdesfilms/ratings.csv')\n",
    "\n",
    "df_3 = pd.merge(df_movies, df_ratings, on='movieId')\n",
    "df_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.1 :</p>\n",
    "\n",
    "Use `df_3` to create a second `DataFrame` with the rating of every movies for each `userId`. If the user never watched a movie the value is $0$. \n",
    "For this step, use the [`pivot` function](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_matrix = ... # TODO 3.1\n",
    "X = user_item_matrix.to_numpy()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.2 :</p>\n",
    "\n",
    "First separate the data using `train_test_split` function with $20\\%$ of the data in the test set.\n",
    "Create an `knn` model using the [`NearestNeighbors`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors) model from `scikit-learn`. For now use $N=80$, and the `cosine` metric, then train it.\n",
    "\n",
    "**Explain in few sentences how does the `cosine` metric works by taking our case in an example (include the formula).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "... = train_test_split(...)\n",
    "\n",
    "knn = ...\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 3.2*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.3 :</p>\n",
    "\n",
    "Create the `predict` function by using the previous trained model.\n",
    "\n",
    "*Hints : Think about the model returns and what they represent. Then, use the closest indexes to obtain information on the best matching movies.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, user_item_matrix, knn):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.4 :</p>\n",
    "\n",
    "Finally, complete the `prec_at_k` function, which measures the precision of our model. This metric calculates how many of the top $k$ movies recommended to a user are actually relevant. A movie is considered relevant if the user has given it a score of at least 4.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_at_k(Y_pred, user_item_matrix, k):\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 3.5 :</p>\n",
    "\n",
    "Now, use the function with three different values for k ($5$, $15$, $25$) each time. Then, use different $N$ neighbours and a different split ratio.\n",
    "\n",
    "Report all your results bellow and explain the impact of these hyperparameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 3.5*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4 - Association rules in online shopping\n",
    "\n",
    "In this fourth part, we will focus on a **Market Basket Analysis** problem. The dataset provided contains **online sales transactions** from an e-commerce site over one year. Our goal is to generate **association rules** based on these sales to identify which items are frequently bought together. You can refer to the source mentioned in the **README file** included with the data for more details.  \n",
    "\n",
    "The dataset is structured as follows, with each row representing the details of a product sale:  \n",
    "- `InvoiceNo`: invoice/sale identifier  \n",
    "- `StockCode`: product identifier  \n",
    "- `Description`: purchased product  \n",
    "- `Quantity`: quantity sold  \n",
    "- `InvoiceDate`: order/payment date  \n",
    "- `UnitPrice`: price of the product  \n",
    "- `CustomerID`: customer identifier  \n",
    "- `Country`: customer’s country of residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.read_excel('data/part4_marketbasketanalysis/OnlineRetailDataset.xlsx', engine='openpyxl')\n",
    "df_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.1 :</p>\n",
    "\n",
    "Transform the given dataset into a one-hot encoded format for association rule mining. Group items by customer so that each row represents a unique customer with a list of purchased items. Then, convert this into a binary matrix where each column is an item, and values indicate whether a customer bought that item (1) or not (0). Use [`MultiLabelBinarizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) to achieve this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.2 :</p>\n",
    "\n",
    "Finally, create the association rules based on the frequent patterns obtained through `FP-growth` algorithm. Check [`mlxtend`](https://rasbt.github.io/mlxtend/) documentation to complete this task. Then, show what are the association rules with a minimum of $0.9$ confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.3 :</p>\n",
    "\n",
    "Do you observe any changes in the different parameters of the *FP-Growth* and *Association Rules* functions ? Please comment on the chosen parameters and the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 4.4 :</p>\n",
    "\n",
    "Is it possible to use other column(s) from the initial data to generate interesting rules?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 4.4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5 - Clustering of mobile apps\n",
    "\n",
    "In this final part, we will perform clustering of applications from the Google Play Store. You can refer to the source mentioned in the **README file** included with the data for more details.\n",
    "\n",
    "Among the attributes available in the `googleplaystore.xlsx` file, we will use the following:\n",
    "\n",
    "* `Rating`: overall user rating of the application\n",
    "* `Reviews`: number of user reviews\n",
    "* `Size`: size of the application\n",
    "* `Installs`: number of users who installed the application\n",
    "* `Price`: price of the application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5 = pd.read_excel('data/part5_clustering/googleplaystore.xlsx', engine='openpyxl')\n",
    "df_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.1 :</p>\n",
    "\n",
    "Select the numerical features of the dataset then standardize them by removing the mean and scaling to unit variance. \n",
    "Then, do a first training using [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) algorithm with $5$ clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.2 :</p>\n",
    "\n",
    "Complete the function `average_cluster_distance` to compute a clustering performance metric. It should return a list where the first value is the global average distance from all samples to their assigned centroids, followed by the average distances per centroid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_cluster_distance(X, model):\n",
    "    ...\n",
    "    return ...\n",
    "\n",
    "avg_distances = average_cluster_distance(...)\n",
    "print(avg_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.3 :</p>\n",
    "\n",
    "Try using different values for the maximum number of iterations of the *K-Means Algorithm*. Then, report your results and describe them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.3*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style='color:red;font-weight:bold'>Exercise 5.4 :</p>\n",
    "\n",
    "Analyze how the category distributions change when clustering with $\\text{n\\_cluster} = {2,3,4,5}$ and compare the results. Observe how categories are grouped within each cluster and note any significant shifts as the number of clusters increases. Describe the distributions obtained and evaluate the average distances for each $\\text{n\\_cluster}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO 5.4*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
